setup_module:
  step1: rm -rf .local.pt*
  step2: source pt1.6.0 mmcv=1.4.6
  step3: cd ~/gml_daily && rm -rf gml
  step4: git clone git@gitlab.sz.sensetime.com:parrotsDL-sz/gml.git
  step5: cd ~/gml_daily/gml && mkdir -p data && cd ~/gml_daily/gml/data
  step6: cp -r ~/ly/data ~/gml_daily/gml/demo/ && cd ~/gml_daily/gml/demo/data/MNIST/raw
  step7: find . -name "*.gz" | xargs gunzip
  step8: cd ~/gml_daily/gml && mkdir .dev && cd .dev
  step9: git clone git@gitlab.sz.sensetime.com:parrotsDL-sz/mmclassification.git
  step10: cd  ~/gml_daily/gml/.dev/mmclassification && export PYTHONPATH=`pwd`:$PYTHONPATH
  step11: git checkout master-opensource
  step12: pip install -e . --user
  step13: cd ~/gml_daily/gml/.dev
  step14: git clone git@gitlab.sz.sensetime.com:parrotsDL-sz/mmdetection.git
  step15: cd ~/gml_daily/gml/.dev/mmdetection && export PYTHONPATH=`pwd`:$PYTHONPATH
  step16: git checkout master-opensource
  step17: pip install -e . --user
  step18: sed -i "s/train2017/val2017/g" ~/gml_daily/gml/configs/_base_/datasets/mmdet/coco_detection.py
  step19: sed -i "s/pat_prototype/pat_dev/g" ~/gml_daily/gml/configs/hpo/gridsearch/search_config.yaml
  step20: sed -i "s/20/2/g" ~/gml_daily/gml/configs/hpo/gridsearch/search_config.yaml
  step21: pip install --index-url https://pkg.sensetime.com/repository/pypi-proxy/simple/
    --extra-index-url http://pavi.parrots.sensetime.com/pypi/simple/ --trusted-host
    pavi.parrots.sensetime.com -U --user pavi
  step22: wait(300s)
  step23: pavi logout -f && pavi login -t 1 -u platform.tester.s.01 -p sense@1234
  step24: cd ~/gml_daily/gml/configs
  step25: python get_data_from_modelhub.py modelhub_model_list.txt ../../
  step26: cd ~/gml_daily/gml/data && ln -s ../../GML_data/gml_unique ./ && ln -s ../../GML_data/gml_checkpoint/
    ./
  step27: cd ~/gml_daily/gml && export PYTHONPATH=`pwd`:$PYTHONPATH
  step28: pip install -e . --user
  step29: pip install -r requirements.txt --user
  step30: cd ~/gml_daily/gml/demo
  step31: python process.py
  step32: search-init
  step33: sed -i "s#ann_file='data/imagenet#ann_file='data/gml_unique/imagenet-1k#g"
    ~/gml_daily/gml/configs/_base_/datasets/mmcls/imagenet_bs64_pil_resize_autoaugv2.py
  step34: sed -i "s#ann_file='data/imagenet#ann_file='data/gml_unique/imagenet-1k#g"
    ~/gml_daily/gml/configs/_base_/datasets/mmcls/imagenet_bs64_pil_resize_autoaugv1.py
  step35: sed -i "s#ann_file='data/imagenet#ann_file='data/gml_unique/imagenet-1k#g"
    ~/gml_daily/gml/configs/_base_/datasets/mmcls/imagenet_bs128_colorjittor.py
  step36: sed -i "s#ann_file='data/imagenet#ann_file='data/gml_unique/imagenet-1k#g"
    ~/gml_daily/gml/configs/_base_/datasets/mmcls/imagenet_bs64_autoformer_224.py
  step37: sed -i "s#ann_file='data/imagenet#ann_file='data/gml_unique/imagenet-1k#g"
    ~/gml_daily/gml/configs/_base_/datasets/mmcls/imagenet_bs128_cream_224.py
  step38: sed -i "s#ann_file=data_root + '#ann_file='data/gml_unique/coco/#g" ~/gml_daily/gml/configs/_base_/datasets/mmdet/detnas_coco_detection.py
  step39: sed -i "s#data_prefix='data/imagenet/#data_prefix='s3://PAT/datasets/project_data/gml_data/gml_unique/imagenet_1k/#g"
    ~/gml_daily/gml/configs/_base_/datasets/mmcls/imagenet_bs64_pil_resize_autoaugv1.py
  step40: sed -i "s#data_prefix='data/imagenet/#data_prefix='s3://PAT/datasets/project_data/gml_data/gml_unique/imagenet_1k/#g"
    ~/gml_daily/gml/configs/_base_/datasets/mmcls/imagenet_bs128_colorjittor.py
  step41: sed -i "s#data_prefix='data/imagenet/#data_prefix='s3://PAT/datasets/project_data/gml_data/gml_unique/imagenet_1k/#g"
    ~/gml_daily/gml/configs/_base_/datasets/mmcls/imagenet_bs64_autoformer_224.py
  step42: sed -i "s#data_prefix='data/imagenet/#data_prefix='s3://PAT/datasets/project_data/gml_data/gml_unique/imagenet_1k/#g"
    ~/gml_daily/gml/configs/_base_/datasets/mmcls/imagenet_bs128_cream_224.py
  step43: sed -i "s#data_prefix='data/imagenet/#data_prefix='s3://PAT/datasets/project_data/gml_data/gml_unique/imagenet_1k/#g"
    ~/gml_daily/gml/configs/_base_/datasets/mmcls/imagenet_bs64_pil_resize_autoaugv2.py
  step44: ln -sf /mnt/lustre/share_data/huangpengsheng/gml_data/cifar10* ~/gml_daily/gml/data
setup_class_hpo:
  step1: cd ${workspace}/demo
case_hpo:
- casename: hpo_gridsearch_search-run -t task
  comment: run a hpo task
  casestep:
    step1: search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml
  except:
    msg1: Task ${{taskname}} finished
- casename: hpo_gridsearch_search-stop
  comment: stop running task
  casestep:
    step1: tmux(search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml)
    step2: wait(180s)
    step3: search-ctl stop -t ${{taskname}}
  except:
    msg1: Kill
- casename: hpo_gridsearch_search-ctl show -t task
  comment: show task
  casestep:
    step1: search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml
    step2: srun -p pat_dev -n 1 search-ctl show -t ${{taskname}}
  except:
    msg1: JobStatus.FINISHED
- casename: hpo_gridsearch_search-ctl export -t task
  comment: export task
  casestep:
    step1: search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml
    step2: search-ctl export -t ${{taskname}} --tar-path ~/gml_daily/gml/${{taskname}}.tar
    step3: cd ~/gml_daily/gml
    step4: setarg(${{EXPORT}},ls | grep *.tar)
  except:
    msg1: ${{taskname}}.tar
- casename: hpo_gridsearch_search-ctl import -t task
  comment: import task
  casestep:
    step1: cd  ~/gml_daily/gml/demo
    step2: search-ctl import -t ${{taskname}} --tar-path  ~/gml_daily/gml/${{EXPORT}}
  except:
    msg1: import ${{taskname}} successfully! load from
- casename: hpo_gridsearch_search-ctl rm -t task
  comment: rm task
  casestep:
    step1: search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml
    step2: tmux(search-ctl rm -t ${{taskname}})
    step3: wait(30s)
    step4: tmux(y)
    step5: wait(60s)
    step6: srun -p pat_dev -n 1 search-ctl show -t ${{taskname}}
  except:
    msg1: ${{taskname}} is not exists
- casename: hpo_gridsearch_search-ctl show all task
  comment: show all tasks
  casestep:
    step1: srun -p pat_dev -n 1 search-ctl show
  except:
    msg1: All tasks
    msg2: finished
- casename: hpo_gridsearch_search-ctl show-scalar
  comment: search-ctl show-scalar
  casestep:
    step1: tmux(search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml)
    step2: wait(1500s)
    step3: srun -p pat_dev -n 1 search-ctl show-scalar -t ${{taskname}} -rt 1
  except:
    msg1: Test Acc
- casename: hpo_gridsearch_search-ctl resume
  casestep:
    step1: tmux(search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml)
    step2: wait(150s)
    step3: search-ctl stop -t ${{taskname}}
    step4: tmux(search-ctl resume -t ${{taskname}})
    step5: wait(80s)
    step6: tmux(y)
    step7: wait(1200s)
    step8: srun -p pat_dev -n 1 search-ctl show -t ${{taskname}}
  except:
    msg1: JobStatus.FINISHED
- casename: hpo_gridsearch_MOCK_PAVI0 search-run
  comment: MOCK_PAVI0 search-run
  casestep:
    step1: MOCK_PAVI=0 search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml
    step2: MOCK_PAVI=0 search-ctl show -t ${{taskname}}
  except:
    msg1: JobStatus.FINISHED
- casename: hpo_gridsearch_MOCK_PAVI1 search-run
  comment: MOCK_PAVI1 search-run
  casestep:
    step1: MOCK_PAVI=1 search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml
    step2: MOCK_PAVI=1 search-ctl show -t ${{taskname}}
  except:
    msg1: JobStatus.FINISHED
    msg2: mock_compare_url
- casename: hpo_gridsearch_MOCK_PAVI2 search-run
  comment: MOCK_PAVI2 search-run
  casestep:
    step1: MOCK_PAVI=2 search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml
    step2: MOCK_PAVI=2 search-ctl show -t ${{taskname}}
  except:
    msg1: JobStatus.FINISHED
    msg2: mock_compare_url
- casename: hpo_gridsearch_MOCK_PAVI3 search-run
  comment: MOCK_PAVI3 search-run
  casestep:
    step1: MOCK_PAVI=3 search-run -t ${{taskname}} ../configs/hpo/gridsearch/search_config.yaml
    step2: MOCK_PAVI=3 search-ctl show -t ${{taskname}}
  except:
    msg1: JobStatus.FINISHED
    msg2: mock_compare_url
case_kd:
- casename: kd_ab_loss_pretrain an epoch
  comment: train an epoch
  casestep:
    step1: GPUS=1 GPUS_PER_NODE=1 bash tools/slurm_train.sh pat_dev fitnet configs/kd/ab_loss/classification/resnet/abloss_res18_cifar10_distillation_8xb16_teacher_res50_backbone_pretrain.py
      work_dirs/abloss_res18_cifar10_distillation_8xb16_teacher_res50_backbone_pretrain/
      --cfg-options runner.max_epochs=1
  except:
    msg1: accuracy_top-1
    msg2: accuracy_top-5
- casename: kd_ab_loss_train an epoch
  comment: train an epoch
  casestep:
    step1: GPUS=1 GPUS_PER_NODE=1 bash tools/slurm_train.sh pat_dev fitnet configs/kd/ab_loss/classification/resnet/abloss_res18_cifar10_distillation_8xb16_teacher_res50_head_train.py
      work_dirs/abloss_res18_cifar10_distillation_8xb16_teacher_res50_head_train/
      --cfg-options runner.max_epochs=1
  except:
    msg1: mme - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_1.pth.
- casename: kd_crd_loss_train a epoch
  comment: train a epoch
  casestep:
    step1: GPUS=1 GPUS_PER_NODE=1 bash tools/slurm_train.sh pat_dev crd configs/kd/crd_loss/classification/resnet/crdloss_res18_cifar10_distillation_8xb16_teacher_res50_dimout128.py
      work_dirs/crdloss_res18_cifar10_distillation_8xb16_teacher_res50_dimout128/
      --cfg-options runner.max_epochs=1
  except:
    msg1: mme - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_1.pth.
- casename: kd_factor_transfer_pretrain an epoch
  comment: pretrain an epoch
  casestep:
    step1: GPUS=1 GPUS_PER_NODE=1 bash tools/slurm_train.sh pat_dev fitnet configs/kd/factor_transfer/classification/resnet/ftloss_res18_cifar10_distillation_8xb16_teacher_res50_neck_pretrain.py
      work_dirs/ftloss_res18_cifar10_distillation_8xb16_teacher_res50_neck_pretrain/
      --cfg-options runner.max_epochs=1
  except:
    msg1: accuracy_top-1
    msg2: accuracy_top-5
- casename: kd_factor_transfer_train an epoch
  comment: train an epoch
  casestep:
    step1: GPUS=1 GPUS_PER_NODE=1 bash tools/slurm_train.sh pat_dev fitnet configs/kd/factor_transfer/classification/resnet/ftloss_res18_cifar10_distillation_8xb16_teacher_res50_neck_train.py
      work_dirs/ftloss_res18_cifar10_distillation_8xb16_teacher_res50_neck_train/
      --cfg-options runner.max_epochs=1
  except:
    msg1: mme - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_1.pth.
- casename: kd_fitnet_train a epoch
  comment: train a epoch
  casestep:
    step1: GPUS=1 GPUS_PER_NODE=1 bash tools/slurm_train.sh pat_dev fitnet configs/kd/fitnet/classification/fitnet_res18_cifar10_distillation_8xb16_teacher_res50_s4_mimic.py
      work_dirs/fitnet_res18_cifar10_distillation_8xb16_teacher_res50_s4_mimic/ --cfg-options
      runner.max_epochs=1
  except:
    msg1: mme - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_1.pth.
- casename: kd_ofd_train a epoch
  comment: train a epoch
  casestep:
    step1: GPUS=1 GPUS_PER_NODE=1 bash tools/slurm_train.sh pat_dev fitnet configs/kd/ofd/classification/resnet/ofdloss_res18_cifar10_distillation_8xb16_teacher_res50_train.py
      work_dirs/ofdloss_res18_cifar10_distillation_8xb16_teacher_res50_train --cfg-options
      runner.max_epochs=1
  except:
    msg1: mme - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_1.pth.
- casename: kd_rkd_train a epoch
  comment: train a epoch
  casestep:
    step1: GPUS=1 GPUS_PER_NODE=1 bash tools/slurm_train.sh pat_dev fitnet configs/kd/rkd/classification/resnet/rkdd_rkda_res18_cifar10_distillation_8xb16_teacher_res50_neck_mimic.py
      work_dirs/rkdd_rkda_res18_cifar10_distillation_8xb16_teacher_res50_neck_mimic
      --cfg-options runner.max_epochs=1
  except:
    msg1: mme - INFO - Now best checkpoint is saved as best_accuracy_top-1_epoch_1.pth.
case_nas:
- casename: nas_darts_darts_supernet_train
  comment: darts_supernet_train
  casestep:
    step1: sed -i "s/max_epochs=50/max_epochs=1/g" ./configs/nas/darts/cifar10_bs16_supernet.py
    step2: GPUS=4 GPUS_PER_NODE=4 tools/slurm_train.sh pat_dev ${{casename}} configs/nas/darts/darts_cellbase_cifar10_supernet_1xb64.py
      work_dirs/darts_supernet_train
    step3: wait(180s)
  except:
    msg1: Search finished
- casename: nas_darts_darts_supernet_test
  comment: darts_supernet_test
  casestep:
    step1: sed -i "s/max_epoch=600/max_epoch=1/g" configs/nas/darts/cifar10_bs96_subnet.py
    step2: setarg(${{EXPORT_YAML}},find work_dirs/darts_supernet_train -name "*.yaml")
    step3: GPUS=4 GPUS_PER_NODE=4 tools/slurm_test.sh pat_dev ${{casename}} configs/nas/darts/darts_cellbase_cifar10_subnet_1xb96.py
      work_dirs/darts_supernet_train/latest.pth --work-dir work_dirs/xxx_test --eval
      accuracy --cfg-options algorithm.mutable_cfg=${{EXPORT_YAML}}
    step4: wait(180s)
  except:
    msg1: accuracy_top-1
    msg2: accuracy_top-5
- casename: nas_bignas_bignas_supernet_train
  comment: bignas_supernet_train
  casestep:
    step1: sed -i "s/max_epochs=360/max_epochs=1/g" ./configs/nas/bignas/bignas_mobilenetv3_large_supernet_32xb64.py
    step2: sed -i "s/samples_per_gpu=64/samples_per_gpu=32/g" ./configs/nas/bignas/bignas_mobilenetv3_large_supernet_32xb64.py
    step3: sed -i "s#/mnt/lustre/share_data/wangshiguang/train_4k.txt#data/gml_unique/imagenet-1k/meta/train.txt#g"
      ./configs/nas/bignas/bignas_mobilenetv3_large_supernet_32xb64.py
    step4: GPUS=4 GPUS_PER_NODE=4 tools/slurm_train.sh pat_dev ${{casename}} configs/nas/bignas/bignas_mobilenetv3_large_supernet_32xb64.py
      work_dirs/bignas_supernet_train
    step5: wait(180s)
  except:
    msg1: Saving checkpoint at 1 epochs
- casename: nas_bignas_bignas_supernet_search
  comment: bignas_supernet_search
  casestep:
    step1: sed -i "s/candidate_pool_size=256/candidate_pool_size=2/g" ./configs/nas/bignas/bignas_mobilenetv3_large_evolution_search_8xb256.py
    step2: sed -i "s/max_epoch=20/max_epoch=1/g" ./configs/nas/bignas/bignas_mobilenetv3_large_evolution_search_8xb256.py
    step3: sed -i "s#/mnt/lustre/share_data/wangshiguang/train_4k.txt#data/gml_unique/imagenet-1k/meta/train.txt#g"
      ./configs/nas/bignas/bignas_mobilenetv3_large_evolution_search_8xb256.py
    step4: sed -i "s#data_prefix='data/imagenet/#data_prefix='s3://PAT/datasets/project_data/gml_data/gml_unique/imagenet_1k/#g"
      ./configs/nas/bignas/bignas_mobilenetv3_large_evolution_search_8xb256.py
    step5: GPUS=4 GPUS_PER_NODE=4 tools/slurm_search.sh pat_dev ${{casename}} configs/nas/bignas/bignas_mobilenetv3_large_evolution_search_8xb256.py
      work_dirs/bignas_supernet_train/latest.pth --work-dir work_dirs/bignas_search
    step6: wait(180s)
  except:
    msg1: Search finished
- casename: nas_bignas_bignas_supernet_test
  comment: bignas_supernet_test
  casestep:
    step1: sed -i "s/max_epoch=360/max_epoch=1/g" ./configs/nas/bignas/bignas_mobilenetv3_large_subnet_8xb128_flops600M.py
    step2: setarg(${{EXPORT_YAML}},find ./work_dirs/bignas_search -name "final_subnet_step5*.yaml")
    step3: setarg(${{EXPORT_PTH}},find ./work_dirs/bignas_search -name "final_subnet_step5*.pth")
    step4: GPUS=4 GPUS_PER_NODE=4 tools/slurm_test.sh pat_dev ${{casename}}  configs/nas/bignas/bignas_mobilenetv3_large_subnet_16xb128_flops600M.py
      ${{EXPORT_PTH}} --work-dir work_dirs/xxx_test --eval accuracy --cfg-options
      algorithm.mutable_cfg=${{EXPORT_YAML}}
    step5: wait(180s)
  except:
    msg1: accuracy_top-1
- casename: nas_spos_spos_supernet_train
  comment: spos_supernet_train
  casestep:
    step1: sed -i "s/max_iters=150000/max_iters=100/g" ./configs/nas/spos/spos_shufflenetv2_supernet_8xb128_in1k.py
    step2: sed -i "s/dict(interval=1000)/dict(interval=100)/g" ./configs/nas/spos/spos_shufflenetv2_supernet_8xb128_in1k.py
    step3: GPUS=4 GPUS_PER_NODE=4 tools/slurm_train.sh pat_dev ${{casename}} configs/nas/spos/spos_shufflenetv2_supernet_8xb128_in1k.py
      work_dirs/spos_supernet
    step4: wait(180s)
  except:
    msg1: Saving checkpoint
- casename: nas_spos_spos_supernet_search
  comment: spos_supernet_search
  casestep:
    step1: setarg(${{EXPORT_PTH}},find ./ -name "latest.pth")
    step2: sed -i "s/candidate_pool_size=50/candidate_pool_size=5/g" ./configs/nas/spos/spos_shufflenetv2_evolution_search_8xb2048_in1k.py
    step3: sed -i "s/candidate_top_k=10/candidate_top_k=2/g" ./configs/nas/spos/spos_shufflenetv2_evolution_search_8xb2048_in1k.py
    step4: sed -i "s/max_epoch=20/max_epoch=2/g" ./configs/nas/spos/spos_shufflenetv2_evolution_search_8xb2048_in1k.py
    step5: sed -i "s/candidate_pool_size=50/candidate_pool_size=5/g" ./configs/nas/spos/spos_shufflenetv2_evolution_search_8xb2048_in1k.py
    step6: sed -i "s/num_mutation=25/num_mutation=2/g" ./configs/nas/spos/spos_shufflenetv2_evolution_search_8xb2048_in1k.py
    step7: sed -i "s/num_crossover=25/num_crossover=3/g" ./configs/nas/spos/spos_shufflenetv2_evolution_search_8xb2048_in1k.py
    step8: GPUS=4 GPUS_PER_NODE=4 tools/slurm_search.sh pat_dev ${{casename}} configs/nas/spos/spos_shufflenetv2_evolution_search_8xb2048_in1k.py
      work_dirs/spos_supernet/latest.pth --work-dir work_dirs/spos_search
    step9: wait(180s)
  except:
    msg1: Search finished
- casename: nas_spos_spos_subnet_retrain
  comment: spos_subnet_retrain
  casestep:
    step1: setarg(${{EXPORT_YAML}},find ./work_dirs/spos_search -name "*.yaml")
    step2: sed -i "s/max_iters=300000/max_iters=100/g" ./configs/nas/spos/spos_shufflenetv2_subnet_8xb128_in1k.py
    step3: GPUS=4 GPUS_PER_NODE=4 tools/slurm_train.sh pat_dev ${{casename}} configs/nas/spos/spos_shufflenetv2_subnet_8xb128_in1k.py
      work_dirs/spos_retrain --cfg-options algorithm.mutable_cfg=${{EXPORT_YAML}}
    step4: wait(180s)
  except:
    msg1: Saving checkpoint
- casename: nas_spos_spos_subnet_test
  comment: spos_subnet_test
  casestep:
    step1: setarg(${{EXPORT_PTH}},find ./work_dirs/spos_retrain -name "latest.pth")
    step2: setarg(${{EXPORT_YAML}},find ./work_dirs/spos_search -name "*.yaml")
    step3: GPUS=4 GPUS_PER_NODE=4 tools/slurm_test.sh pat_dev ${{casename}} configs/nas/spos/spos_shufflenetv2_subnet_8xb128_in1k.py
      ${{EXPORT_PTH}} --work-dir work_dirs/spos_test --eval accuracy --cfg-options
      algorithm.mutable_cfg=${{EXPORT_YAML}}
    step4: wait(180s)
  except:
    msg1: accuracy_top-1
- casename: nas_zennas_zennas_supernet_search
  comment: zennas_supernet_search
  casestep:
    step1: sed -i "s/num_crossover=128/num_crossover=3/g" ./configs/nas/zennas/mobilenetv3_zenscore_evolution_search_8xb256.py
    step2: sed -i "s/candidate_pool_size=256/candidate_pool_size=5/g" ./configs/nas/zennas/mobilenetv3_zenscore_evolution_search_8xb256.py
    step3: sed -i "s/candidate_top_k=256/candidate_top_k=2/g" ./configs/nas/zennas/mobilenetv3_zenscore_evolution_search_8xb256.py
    step4: sed -i "s/max_epoch=20/max_epoch=2/g" ./configs/nas/zennas/mobilenetv3_zenscore_evolution_search_8xb256.py
    step5: sed -i "s/num_mutation=128/num_mutation=2/g" ./configs/nas/zennas/mobilenetv3_zenscore_evolution_search_8xb256.py
    step6: GPUS=4 GPUS_PER_NODE=4 tools/slurm_search.sh pat_dev ${{casename}} configs/nas/zennas/mobilenetv3_zenscore_evolution_search_8xb256.py
      data/gml_checkpoint/0310_bignas_latest.pth --work-dir work_dirs/zennas_supernet_search
    step7: wait(180s)
  except:
    msg1: Search finished
- casename: nas_zennas_zennas_supernet_retrain
  comment: zennas_supernet_retrain
  casestep:
    step1: sed -i "s#efficientnet-b3_8xb32_in1k_timm_84_01_v3.pth#~/gml_daily/gml/data/gml_checkpoint/efficientnet-b3_8xb32_in1k_timm_84_01_v3.pth#g"
      configs/nas/zennas/EXP_3_E3tozennet0.1_distillation_8xb64_teacher_mimic_ns_connnector.py
    step2: sed -i "s/max_epochs=480/max_epochs=1/g" configs/nas/zennas/EXP_3_E3tozennet0.1_distillation_8xb64_teacher_mimic_ns_connnector.py
    step3: GPUS=4 GPUS_PER_NODE=4 tools/slurm_train.sh pat_dev ${{casename}} configs/nas/zennas/EXP_3_E3tozennet0.1_distillation_8xb64_teacher_mimic_ns_connnector.py
      work_dirs/${{casename}}
    step4: wait(180s)
  except:
    msg1: Saving checkpoint
- casename: nas_cream_cream_supernet_train
  comment: cream_supernet_train
  casestep:
    step1: sed -i "s/max_epochs=120/max_epochs=2/g" ./configs/nas/cream/cream_mobilenetv3_supernet_8xb128_max500M.py
    step2: sed -i "s/meta_sta_epoch=20/meta_sta_epoch=-1/g" ./configs/nas/cream/cream_mobilenetv3_supernet_8xb128_max500M.py
    step3: sed -i "s/samples_per_gpu=128/samples_per_gpu=64/g" ./configs/_base_/datasets/mmcls/imagenet_bs128_cream_224.py
    step4: GPUS=4 GPUS_PER_NODE=4 tools/slurm_train.sh pat_dev ${{casename}} configs/nas/cream/cream_mobilenetv3_supernet_8xb128_max500M.py
      work_dirs/cream_supernet
    step5: wait(180s)
  except:
    msg1: Saving checkpoint
- casename: nas_cream_cream_subnet_retrain
  comment: cream_subnet_retrain
  casestep:
    step1: setarg(${{EXPORT_YAML}},find ./work_dirs/cream_supernet -name "*.yaml")
    step2: sed -i "s/max_epochs=530/max_epochs=2/g" ./configs/nas/cream/cream_mobilenetv3_subnet_16xb128_max500M.py
    step3: sed -i "s/samples_per_gpu=128/samples_per_gpu=64/g" ./configs/_base_/datasets/mmcls/imagenet_bs128_cream_224.py
    step4: GPUS=4 GPUS_PER_NODE=4 tools/slurm_train.sh pat_dev ${{casename}} configs/nas/cream/cream_mobilenetv3_subnet_16xb128_max500M.py
      work_dirs/cream_retrain --cfg-options algorithm.mutable_cfg=${{EXPORT_YAML}}
    step5: wait(180s)
  except:
    msg1: Saving checkpoint
- casename: nas_cream_cream_subnet_test
  comment: cream_subnet_test
  casestep:
    step1: setarg(${{EXPORT_PTH}},find ./work_dirs/cream_retrain -name "latest.pth")
    step2: setarg(${{EXPORT_YAML}},find ./work_dirs/cream_supernet -name "*.yaml")
    step3: GPUS=4 GPUS_PER_NODE=4 tools/slurm_test.sh pat_dev ${{casename}} configs/nas/cream/cream_mobilenetv3_subnet_16xb128_max500M.py
      ${{EXPORT_PTH}} --eval accuracy --cfg-options algorithm.mutable_cfg=${{EXPORT_YAML}}
    step4: wait(180s)
  except:
    msg1: accuracy_top-1
- casename: nas_nsganet_nsganet_supernet_train
  comment: nsganet_supernet_train
  casestep:
    step1: sed -i "s/max_epochs=1/max_epochs=2/g" ./configs/nas/nsganet/imagenet/nsganet_supernet_8xb256.py
    step2: setarg(${{SUPERNET_FILE}},find ./configs/nas/nsganet/imagenet/ -name "nsganet_supernet_*.py")
    step3: GPUS=4 GPUS_PER_NODE=4 tools/slurm_train.sh pat_dev ${{casename}} ${{SUPERNET_FILE}}
      work_dirs/nsganet_supernet
    step4: wait(180s)
  except:
    msg1: Saving checkpoint
case_pruning:
- casename: pruning_dmcp_dmcp_mbv2_train
  comment: dmcp_mbv2_train
  casestep:
    step1: sed -i "s/max_epochs=60/max_epochs=1/g" ./configs/pruning/dmcp/dmcp_mobilenetv2_imagenet_8xb256_f150.py
    step2: sed -i "s/target_flops=150/target_flops=315/g" ./configs/pruning/dmcp/dmcp_mobilenetv2_imagenet_8xb256_f150.py
    step3: sed -i "s/distill_iter=2000/distill_iter=10/g" ./configs/pruning/dmcp/dmcp_mobilenetv2_imagenet_8xb256_f150.py
    step4: sed -i "s/arch_start_train=None/arch_start_train=2000/g" ./configs/pruning/dmcp/dmcp_mobilenetv2_imagenet_8xb256_f150.py
    step5: sed -i "s/samples_per_gpu=256/samples_per_gpu=128/g" ./configs/_base_/datasets/mmcls/imagenet_bs256_autoslim.py
    step6: GPUS=1 GPUS_PER_NODE=1 tools/slurm_train.sh pat_dev ${{casename}} configs/pruning/dmcp/dmcp_mobilenetv2_imagenet_8xb256_f150.py
      work_dirs/dmcp_mbv2_train
  except:
    msg1: Saving checkpoint at 1 epochs
- casename: pruning_dmcp_dmcp_mbv2_retrain
  comment: dmcp_mbv2_retrain
  casestep:
    step1: sed -i "s/max_epochs=60/max_epochs=1/g" ./configs/pruning/dmcp/dmcp_mobilenetv2_imagenet_8xb256_retrain.py
    step2: sed -i "s#/DMCP_WORKDIR/model_sample/subnet.npy#work_dirs/dmcp_mbv2_train/model_sample/subnet_1.npy#g"
      ./configs/pruning/dmcp/dmcp_mobilenetv2_imagenet_8xb256_retrain.py
    step3: GPUS=1 GPUS_PER_NODE=1 tools/slurm_train.sh pat_dev ${{casename}} configs/pruning/dmcp/dmcp_mobilenetv2_imagenet_8xb256_retrain.py
      work_dirs/dmcp_mbv2_retrain
  except:
    msg1: Retrain finished
- casename: pruning_dcff_dcff_resnet50
  comment: dcff_prune_resnet50_for_1_epoch
  casestep:
    step1: sed -i "s/max_epochs=120/max_epochs=1/g" ./configs/pruning/dcff/classification/dcff_resnet50_imagenet_8xb32.py
    step2: GPUS=1 GPUS_PER_NODE=1 tools/slurm_train.sh pat_dev ${{casename}} configs/pruning/dcff/classification/dcff_resnet50_imagenet_8xb32.py
      work_dirs/dcff_resnet50
  except:
    msg1: Train finished and save checkpoint at 1 epochs
